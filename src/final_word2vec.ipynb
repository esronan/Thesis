{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c77182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install gensim\n",
    "#!pip install mgzip\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim import downloader\n",
    "import gzip \n",
    "import math\n",
    "import itertools\n",
    "from time import time\n",
    "import os\n",
    "import tqdm as tq\n",
    "import mgzip\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953a0bf",
   "metadata": {},
   "source": [
    "# Set up loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f77a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        logging.debug('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        log_fil.write('Loss after epoch {}: {}\\n\\n'.format(self.epoch, loss_now))\n",
    "        losses[filname][self.epoch] = loss_now\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2886f",
   "metadata": {},
   "source": [
    "# Ngram generator from gzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4053141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngram_extractor_lite():\n",
    "    def __init__(self, filname, blank=False, limit=None):\n",
    "        self.limit = limit\n",
    "        self.filname = filname\n",
    "        self.blank = blank\n",
    "    def __iter__(self):\n",
    "        start = time()\n",
    "        print(\"n\", self.filname)\n",
    "        with gzip.open(self.filname, \"rt\", encoding=\"ISO-8859-1\") as fil: #just changed from r to rt so that it reads the csv separations\n",
    "        # with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "            j = 0\n",
    "            for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                #j+=1\n",
    "                #if j > self.blank:\n",
    "                 #   break\n",
    "                line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                ngram = line[0].split()\n",
    "                if len(ngram) < 3:\n",
    "                    continue\n",
    "                try:\n",
    "                    match_count = int(line[-1])\n",
    "                except:\n",
    "                    continue\n",
    "                #prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                for i in range(match_count):\n",
    "                    yield ngram\n",
    "            t = time()-start\n",
    "            print(f\"Time taken: {t // 3600}H, {round((t % 3600)/60, 2)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2sent(fil):\n",
    "    f = open(fil, \"r\")\n",
    "    sents = []\n",
    "    for line in tq.tqdm(itertools.islice(f)):\n",
    "        line = gensim.utils.to_unicode(line)\n",
    "        \n",
    "    return sents\n",
    "os.chdir(\"D:/google_ngrams/processed_data/business_text\")\n",
    "f = open(os.listdir()[0], \"r\").readlines()\n",
    "sents = []\n",
    "i = 0\n",
    "for line in tq.tqdm(f):\n",
    "    line = gensim.utils.to_unicode(line)\n",
    "    print(line[:150])\n",
    "    i +=1\n",
    "    if i > 10:\n",
    "        break\n",
    "print(os.listdir()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a360304",
   "metadata": {},
   "source": [
    "# Find where in the files the blanks (Pure POS tags) begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcce1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_blanks(dir):\n",
    "    ''' Create two dictionaries, one containing the index of the first blank line in each file,\n",
    "    and the second containing the length of the files'''\n",
    "    if dir[-1] != \"/\":\n",
    "        print(\"Add slash to end of dir.\")\n",
    "        return None\n",
    "    filnames = [f\"19{i}0.gz\" for i in range(0,10)]\n",
    "    infos = {\"blank_ind\": {}, \"length\": {}}\n",
    "    os.chdir(dir)\n",
    "    try:\n",
    "        infos = pd.read_csv(dir + \"file_infos.csv\")\n",
    "        infos = infos.set_index(\"filename\")\n",
    "        print(\"Infos loaded.\")\n",
    "        return infos\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        (\"Calculating infos...\")\n",
    "        for fil in filnames[:1]:\n",
    "            print(fil)\n",
    "            with gzip.open(fil, \"rt\", encoding = 'utf-8') as f:\n",
    "                lst = []\n",
    "                for line in f:\n",
    "                    spl = line.split()\n",
    "                    lst.append(spl)\n",
    "            for i,spl in enumerate(lst[::-1]):\n",
    "                #print(\"spl\", spl, \"eln\", len(spl))\n",
    "                if len(spl) > 2:\n",
    "                    infos[\"blank_ind\"][fil] = len(lst)-(i+1)\n",
    "                    infos[\"length\"][fil] = len(lst)\n",
    "                    break\n",
    "        infos = pd.DataFrame(infos)#.to_csv(dir + \"file_infos\")\n",
    "        infos.index.name=\"filename\"\n",
    "        print(infos)\n",
    "        infos.to_csv(dir + \"file_infos.csv\")\n",
    "    return infos\n",
    "infos = find_blanks(\"D:/google_ngrams/processed_data/gb_12_full_full/\")\n",
    "#TO ADD: ngram counts, averages, etc - everythign in expoloratory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#infos = infos.set_index(\"filename\")\n",
    "infos[\"blank_ind\"][\"1900.gz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9187e2",
   "metadata": {},
   "source": [
    "# Iterable version (no loss calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7941dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1 # check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\n",
    "\n",
    "#an iterable version, so each iteration is ssaved (perhaps for each file there should be a save?):\n",
    "epochs = 5\n",
    "vocab = False\n",
    "completed = 0\n",
    "method = 1 # 1  sg, cbow = 0\n",
    "\n",
    "size = 300 # vector size\n",
    "min_count = 25\n",
    "filnames = [f\"19{i}0.gz\" for i in range(0,10)]\n",
    "\n",
    "os.chdir(\"D:/google_ngrams/processed_data/gb_12_full_full\")\n",
    "\n",
    "for filname in filnames[:1]:\n",
    "    ngrams = ngram_extractor_lite(filname=filname)#, blank=infos[\"blank_id\"][filname])\n",
    "    for i in range(epochs-completed):\n",
    "        if vocab:\n",
    "            model = Word2Vec.load('w2vmodel_ng5_' + filname  + \"_\" + str(\"vocab\"))\n",
    "            print(\"Vocab loaded.\")\n",
    "        elif completed != 0:\n",
    "            model = Word2Vec.load('w2vmodel_ng5_' + filname + \"_\" + str(completed))\n",
    "            print(\"Model loaded.\")\n",
    "        elif i == 0:\n",
    "            model = gensim.models.word2vec.Word2Vec(sg=method, vector_size=size, window=5, sg=1 min_count=25, workers=10, hs=0, negative=8, epochs=1, sample=10000, compute_loss=True, callbacks = [callback()])\n",
    "            model.build_vocab(ngrams)\n",
    "            print(\"Vocab built.\")\n",
    "            model.save('w2vmodel_ng5_'+ filname +'_' + str(\"vocab\"))\n",
    "        model.train(ngrams, total_examples = model.corpus_count, epochs = 1)\n",
    "        model.save('w2vmodel_ng5_'+ filname +'_' + str(completed+i+1))\n",
    "        print(\"Epoch \" + str(completed + i + 1) + \" completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43f560",
   "metadata": {},
   "source": [
    "# Standard version - with loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a0099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"D:/google_ngrams/processed_data/business_text\")\n",
    "output_dir = \"D:/google_ngrams/Vectors/business_vectors\"\n",
    "\n",
    "#filnames = [f\"19{i}0.gz\" for i in range(0,10)] \n",
    "colls =[\"economist2\", \"forbes\", \"bloomberg\", \"fortune\", \"hbr\"]\n",
    "filnames = [\"first\", \"second\", \"third\"]\n",
    "\n",
    "#filnames = [f\"first_spirit_{coll}\" for coll in colls]\n",
    "losses = {filname: {} for filname in filnames}\n",
    "for filname in filnames:\n",
    "    logging.basicConfig(filename=f'{output_dir}/{filname}.log', encoding='utf-8', level=logging.DEBUG)\n",
    "    log_fil = open(f'{output_dir}/{filname}_log.txt', mode = 'w')\n",
    "    #ngrams = ngram_extractor_lite(filname=filname)#, blank=infos[\"blank_ind\"][filname])\n",
    "    txts = [f\"{filname}_spirit_{coll}.txt\" for coll in colls]\n",
    "    ngrams = []\n",
    "    for txt in txts:\n",
    "        content = file2sent(txt)\n",
    "        ngrams += content\n",
    "        print(ngrams[:10])\n",
    "        del content\n",
    "    print(\"Starting training...\")\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, vector_size=300, window=5, min_count=5, compute_loss=True, callbacks = [callback()], workers=10, hs=0, negative=8) \n",
    "    #default epochs = 5, size = vector dimensions, min_count= min count for word to be considered, workers for multiprocessing, sg=1 = skipgram (0 = CBOW), hs=1 = softmax used, negative = no. noise words used in negative sampling\n",
    "    model.save(output_dir + \"w2vmodel_ng5_sg_\" + filname +'_full')\n",
    "    pd.DataFrame(losses).to_csv(f\"{output_dir}/{filname}_losses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0af3236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "n 1990.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1126000it [00:07, 155381.26it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Error -3 while decompressing data: invalid block type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_8760/1838687482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#    del content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting training...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;31m#default epochs = 5, size = vector dimensions, min_count= min count for word to be considered, workers for multiprocessing, sg=1 = skipgram (0 = CBOW), hs=1 = softmax used, negative = no. noise words used in negative sampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"w2vmodel_ng5_sg_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilname\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'_full'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m             self.train(\n\u001b[0;32m    427\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m         \"\"\"\n\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[0;32m    488\u001b[0m             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         logger.info(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_8760/1283212099.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfil\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[1;31m#j+=1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;31m#if j > self.blank:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread1\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m             \u001b[0muncompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: Error -3 while decompressing data: invalid block type"
     ]
    }
   ],
   "source": [
    "os.chdir(\"D:/google_ngrams/processed_data/us_12_final_burst\")\n",
    "output_dir = \"D:/google_ngrams/Vectors/us_12_final/\"\n",
    "\n",
    "filnames = [f\"19{i}0.gz\" for i in range(0,10)] \n",
    "#colls =[\"economist2\", \"forbes\", \"bloomberg\", \"fortune\", \"hbr\"]\n",
    "#filnames = [\"first\", \"second\", \"third\"]\n",
    "\n",
    "#filnames = [f\"first_spirit_{coll}\" for coll in colls]\n",
    "losses = {filname: {} for filname in filnames}\n",
    "for filname in filnames[::-1]:\n",
    "   # logging.basicConfig(filename=f'{output_dir}/{filname}.log', encoding='utf-8', level=logging.DEBUG)\n",
    "    log_fil = open(f'{output_dir}/{filname}_log.txt', mode = 'w')\n",
    "    ngrams = ngram_extractor_lite(filname=filname)#, blank=infos[\"blank_ind\"][filname])\n",
    "    #txts = [f\"{filname}_spirit_{coll}.txt\" for coll in colls]\n",
    "    #ngrams = []\n",
    "    #for txt in txts:\n",
    "    #    content = file2sent(txt)\n",
    "    #    ngrams += content\n",
    "    #    print(ngrams[:10])\n",
    "    #    del content\n",
    "    print(\"Starting training...\")\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, vector_size=300, window=5, min_count=5, compute_loss=True, callbacks = [callback()], workers=10, hs=0, negative=8) \n",
    "    #default epochs = 5, size = vector dimensions, min_count= min count for word to be considered, workers for multiprocessing, sg=1 = skipgram (0 = CBOW), hs=1 = softmax used, negative = no. noise words used in negative sampling\n",
    "    model.save(output_dir + \"w2vmodel_ng5_sg_\" + filname +'_full')\n",
    "    pd.DataFrame(losses).to_csv(f\"{output_dir}/{filname}_losses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002504a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
