{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating list of URLS per ngram collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import re as reg\n",
    "#Creating a list of URLs for each of the Google Books corpora\n",
    "ngram_urls = {}\n",
    "\n",
    "#2020 English - all (both British & American) \n",
    "num_files = 19423\n",
    "ngram_urls[\"eng_all_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng/5-{}-of-19423.gz\".format(i)\n",
    "  for i in range(num_files)]\n",
    "\n",
    "#2012 english \n",
    "alph_list = []\n",
    "num_files = 1000\n",
    "alph = \"_\" + string.ascii_lowercase\n",
    "\n",
    "\n",
    "for i in range(num_files)[27:]: \n",
    "    try:\n",
    "        alph_list.append(alph[i//27] + alph[i%27])\n",
    "    except:\n",
    "        pass\n",
    "#All suffixes - lacking \n",
    "#turn below into a list and add the above\n",
    "all_suff = alph_list + list(string.digits) + [\"_ADJ_\", \"_ADP_\", \"_ADV_\", \"_CONJ_\", \"_DET_\", \"_NOUN_\", \"_NUM_\", \"_PRON_\", \"_PRT_\", \"_VERB_\"]       \n",
    "\n",
    "#All english 2012\n",
    "ngram_urls[\"eng_all_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n",
    "\n",
    "#English one million\n",
    "ngram_urls[\"one_mill\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-1M-5gram-20090715-{}.csv.zip\".format(i) for i in range(799)]\n",
    "\n",
    "#British English 2020\n",
    "ngram_urls[\"gb_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-gb/5-{}-of-03098.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(3098)]\n",
    "\n",
    "#British English 2020\n",
    "ngram_urls[\"gb_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-gb/5-{}-of-03098.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(3098)]\n",
    "\n",
    "#British English 2012\n",
    "ngram_urls[\"gb_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-gb-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n",
    "\n",
    "#American English 2020\n",
    "ngram_urls[\"us_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-us/5-{}-of-11145.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(11145)]\n",
    "\n",
    "#American English 2012\n",
    "ngram_urls[\"us_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-us-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n",
    "\n",
    "#English Fiction 2020\n",
    "ngram_urls[\"eng_fic_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-fiction/5-{}-of-01449.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(1449)]\n",
    "\n",
    "#English Fiction 2012\n",
    "ngram_urls[\"eng_fic_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-fiction-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n",
    "\n",
    "ngram_urls[\"us_12_1gram\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-us-all-5gram-20120701-{}.gz\".format(i) for i in range(10)] \n",
    "ngram_urls[\"us_12_1gram\"] =  ngram_urls[\"us_12_1gram\"] + [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-us-all-5gram-20120701-{}.gz\".format(i) for i in string.ascii_lowercase]\n",
    "    \n",
    "ngram_urls[\"gb_12_1gram\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-us-all-5gram-20120701-{}.gz\".format(i) for i in range(10)] \n",
    "ngram_urls[\"gb_12_1gram\"] = ngram_urls[\"gb_12_1gram\"] + [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-us-all-5gram-20120701-{}.gz\".format(i) for i in string.ascii_lowercase]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to download a specified collection (coll) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/0\n",
      "D:/google_ngrams/raw_data/us_12_1gram/0\n",
      "Iteration: 0 of 36, time taken: 3.090895891189575\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/1\n",
      "D:/google_ngrams/raw_data/us_12_1gram/1\n",
      "Iteration: 1 of 36, time taken: 45.82982540130615\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/2\n",
      "D:/google_ngrams/raw_data/us_12_1gram/2\n",
      "Iteration: 2 of 36, time taken: 63.74636960029602\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/3\n",
      "D:/google_ngrams/raw_data/us_12_1gram/3\n",
      "Iteration: 3 of 36, time taken: 73.0505485534668\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/4\n",
      "D:/google_ngrams/raw_data/us_12_1gram/4\n",
      "Iteration: 4 of 36, time taken: 82.04801535606384\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/5\n",
      "D:/google_ngrams/raw_data/us_12_1gram/5\n",
      "Iteration: 5 of 36, time taken: 91.28902339935303\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/6\n",
      "D:/google_ngrams/raw_data/us_12_1gram/6\n",
      "Iteration: 6 of 36, time taken: 98.81421184539795\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/7\n",
      "D:/google_ngrams/raw_data/us_12_1gram/7\n",
      "Iteration: 7 of 36, time taken: 103.73463535308838\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/8\n",
      "D:/google_ngrams/raw_data/us_12_1gram/8\n",
      "Iteration: 8 of 36, time taken: 108.48769545555115\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/9\n",
      "D:/google_ngrams/raw_data/us_12_1gram/9\n",
      "Iteration: 9 of 36, time taken: 112.39631485939026\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/a\n",
      "D:/google_ngrams/raw_data/us_12_1gram/a\n",
      "Iteration: 10 of 36, time taken: 112.58217310905457\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/b\n",
      "D:/google_ngrams/raw_data/us_12_1gram/b\n",
      "Iteration: 11 of 36, time taken: 112.77640986442566\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/c\n",
      "D:/google_ngrams/raw_data/us_12_1gram/c\n",
      "Iteration: 12 of 36, time taken: 112.97193503379822\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/d\n",
      "D:/google_ngrams/raw_data/us_12_1gram/d\n",
      "Iteration: 13 of 36, time taken: 113.15362572669983\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/e\n",
      "D:/google_ngrams/raw_data/us_12_1gram/e\n",
      "Iteration: 14 of 36, time taken: 113.33914637565613\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/f\n",
      "D:/google_ngrams/raw_data/us_12_1gram/f\n",
      "Iteration: 15 of 36, time taken: 113.53204560279846\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/g\n",
      "D:/google_ngrams/raw_data/us_12_1gram/g\n",
      "Iteration: 16 of 36, time taken: 113.71515607833862\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/h\n",
      "D:/google_ngrams/raw_data/us_12_1gram/h\n",
      "Iteration: 17 of 36, time taken: 113.90833044052124\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/i\n",
      "D:/google_ngrams/raw_data/us_12_1gram/i\n",
      "Iteration: 18 of 36, time taken: 114.08190655708313\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/j\n",
      "D:/google_ngrams/raw_data/us_12_1gram/j\n",
      "Iteration: 19 of 36, time taken: 114.25542426109314\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/k\n",
      "D:/google_ngrams/raw_data/us_12_1gram/k\n",
      "Iteration: 20 of 36, time taken: 114.4269654750824\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/l\n",
      "D:/google_ngrams/raw_data/us_12_1gram/l\n",
      "Iteration: 21 of 36, time taken: 114.61000084877014\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/m\n",
      "D:/google_ngrams/raw_data/us_12_1gram/m\n",
      "Iteration: 22 of 36, time taken: 114.80230808258057\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/n\n",
      "D:/google_ngrams/raw_data/us_12_1gram/n\n",
      "Iteration: 23 of 36, time taken: 114.98969316482544\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/o\n",
      "D:/google_ngrams/raw_data/us_12_1gram/o\n",
      "Iteration: 24 of 36, time taken: 115.19115686416626\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/p\n",
      "D:/google_ngrams/raw_data/us_12_1gram/p\n",
      "Iteration: 25 of 36, time taken: 115.37536430358887\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/q\n",
      "D:/google_ngrams/raw_data/us_12_1gram/q\n",
      "Iteration: 26 of 36, time taken: 115.57168316841125\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/r\n",
      "D:/google_ngrams/raw_data/us_12_1gram/r\n",
      "Iteration: 27 of 36, time taken: 115.7501630783081\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/s\n",
      "D:/google_ngrams/raw_data/us_12_1gram/s\n",
      "Iteration: 28 of 36, time taken: 115.93370294570923\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/t\n",
      "D:/google_ngrams/raw_data/us_12_1gram/t\n",
      "Iteration: 29 of 36, time taken: 116.11843848228455\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/u\n",
      "D:/google_ngrams/raw_data/us_12_1gram/u\n",
      "Iteration: 30 of 36, time taken: 116.31913948059082\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/v\n",
      "D:/google_ngrams/raw_data/us_12_1gram/v\n",
      "Iteration: 31 of 36, time taken: 116.51102590560913\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/w\n",
      "D:/google_ngrams/raw_data/us_12_1gram/w\n",
      "Iteration: 32 of 36, time taken: 116.69596672058105\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/x\n",
      "D:/google_ngrams/raw_data/us_12_1gram/x\n",
      "Iteration: 33 of 36, time taken: 116.89309072494507\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/y\n",
      "D:/google_ngrams/raw_data/us_12_1gram/y\n",
      "Iteration: 34 of 36, time taken: 117.0968964099884\n",
      "\n",
      "Starting:  D:/google_ngrams/raw_data/us_12_1gram/z\n",
      "D:/google_ngrams/raw_data/us_12_1gram/z\n",
      "Iteration: 35 of 36, time taken: 117.28265500068665\n",
      "Total time to download: 117.28365802764893\n"
     ]
    }
   ],
   "source": [
    "#https://likegeeks.com/downloading-files-using-python/\n",
    "#PBAR: https://towardsdatascience.com/how-to-download-files-using-python-part-2-19b95be4cdb5\n",
    "import requests as re\n",
    "from time import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "# from tqdm import tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "# url = \"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-5gram-20120701-a_.gz\"\n",
    "# file = re.get(url)\n",
    "# open(file).write(file.content)\n",
    "\n",
    "#CHANGE THE PATH TO FOLDER ON HARDDRIVE\n",
    "def url_dl(coll, url, path, it):\n",
    "  #  if coll[-2:] == \"12\":\n",
    "        #name = reg.search(\"01-.+.gz\", url).start()\n",
    "    path = path + url[-4:-3]\n",
    "        \n",
    "    print(f\"\\nStarting: \",path)\n",
    "    r = re.get(url, stream=True)\n",
    "#     r.raise_for_status()\n",
    "    total_size = int(r.headers.get('content-length'))\n",
    "    initial_pos = 0\n",
    "    print(path)\n",
    "    with open(path, \"wb\") as f: \n",
    "        for chunk in r.iter_content(chunk_size=8192):  \n",
    "            f.write(chunk)\n",
    "    del r\n",
    "\n",
    "hd_path = \"D:/google_ngrams/raw_data/\"\n",
    "start = time()\n",
    "\n",
    "\n",
    "coll = \"us_12_1gram\"\n",
    "os.makedirs(hd_path + coll + \"/\")\n",
    "\n",
    "current_iter= 0\n",
    "for i,url in enumerate(ngram_urls[coll][current_iter:]):\n",
    "    file_path = hd_path + coll + \"/\"\n",
    "    url_dl(coll, url, file_path, str(i))\n",
    "    print(f\"Iteration: {i+current_iter} of {len(ngram_urls[coll])}, time taken: {time() - start}\")\n",
    "\n",
    "# iterating through all collections\n",
    "# for coll in colls:\n",
    "#     for i,url in enumerate(ngram_urls[coll]):\n",
    "#         print(f\"Iteration: {i} of {len(ngram_urls[coll])}, time taken: {time() - start}\")\n",
    "#         file_path = hd_path + coll + \"/\"\n",
    "#         url_dl(url, file_path)\n",
    "\n",
    "print(f\"Total time to download: {time() - start}\") # I think this will just print out at end?\n",
    "# ThreadPool(9).imap_unordered(url_dl, ngram_urls[\"one_mill\"][:100]) #for multiple URLs\n",
    "#urllib.request.urlretrieve('url', 'path') using urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n",
    "# r.raise_for_status()\n",
    "#         with open(local_filename, 'wb') as f:\n",
    "#             for chunk in r.iter_content(chunk_size=8192): \n",
    "#                 # If you have chunk encoded response uncomment if\n",
    "#                 # and set chunk_size parameter to None.\n",
    "#                 #if chunk: \n",
    "#                 f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset sizes \n",
    "- English One Million: 31.98 gig\n",
    "- eng_all_20: ?\n",
    "- gb_20: 1845gb\n",
    "- us_20: ?\n",
    "- eng_all_12: 3.67 gb\n",
    "- us12: 160gb\n",
    "- gb12: 52.6gb. Time to download: 2126.120045900345\n",
    "- eng_fic20: 659gb\n",
    "- eng_fic12: 23.4gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to find size of each collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests as re\n",
    "\n",
    "# sum([re.head(\"http://storage.googleapis.com/books/ngrams/books/20200217/eng/5-1-of-19423.gz\")[\"Content-length\"] for url in ngram_urls[\"one_mill\"]])\n",
    "size = 0\n",
    "for i, url in enumerate(ngram_urls[\"gb_12\"]):\n",
    "    \n",
    "    f = re.head(url)\n",
    "    size = size + int(f.headers[\"Content-length\"])\n",
    "    if i == len(ngram_urls[\"gb_12\"])-1:\n",
    "        print(i)\n",
    "        print(size/10**9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
