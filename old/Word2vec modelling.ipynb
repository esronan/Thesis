{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial example\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'word2vec.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m             \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved %s object\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-2b430df87665>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Input is a seires of n-grams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word2vec.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# model = Word2Vec.load(\"word2vec.model\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m         \"\"\"\n\u001b[1;32m-> 1900\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved %s object\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    608\u001b[0m         )\n\u001b[0;32m    609\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[0mpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[1;31m# restore attribs handled specially\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mpickle\u001b[1;34m(obj, fname, protocol)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m     \"\"\"\n\u001b[1;32m-> 1441\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'word2vec.model'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim import downloader\n",
    "import gzip \n",
    "import math\n",
    "import itertools\n",
    "\n",
    "#Input is a seires of n-grams\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "# model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
    "ex_vector = model.wv['computer'] \n",
    "sims = model.wv.most_similar('computer', topn=10) \n",
    "\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n",
    "\n",
    "# Load back with memory-mapping = read-only, shared across processes.\n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "vector = wv['computer']  # Get numpy vector of a word\n",
    "\n",
    "del model # to alleviate pressure from memory, using the vectors instead (as I understand it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model to recognise phrases\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Train a bigram detector.\n",
    "bigram_transformer = Phrases(common_texts)\n",
    "\n",
    "# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
    "model = Word2Vec(bigram_transformer[common_texts], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glove_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8ebfa28c9719>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# glove_vectors = gensim.downloader.load('glove-twitter-25')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mglove_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hello\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "#Pre trained models\n",
    "pre_trained = list(gensim.downloader.info()['models'].keys())\n",
    "print(pre_trained)\n",
    "# glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "# glove_vectors.most_similar(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first attempt - partitioning the data according to year (is this a good preliminary step to do before attempting the collab? It will easen the load on the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "#example file\n",
    "ng = \"googlebooks-eng-all-5gram-20120701-gp.gz\"\n",
    "\n",
    "# first try at opening\n",
    "# f = gzip.open(ng, \"rb\")\n",
    "# contents = f.readlines()\n",
    "# for c in contents:\n",
    "#     c.split(\"\\t\")\n",
    "\n",
    "#Using pandas, first is if you want to chunk data\n",
    "# for chunk in pd.read_csv(ng, sep='\\t', chunksize=10**5):\n",
    "#     #process\n",
    "colnames = [\"ngram\", \"year\", \"match_count\", \"vol_count\"]\n",
    "df = pd.read_csv(ng, sep='\\t', names=colnames, header=None )\n",
    "#[index] ngram TAB year TAB match_count TAB volume_count NEWLINE\n",
    "\n",
    "\n",
    "ybins = [1800+i*10 for i in range(22)]\n",
    "ylabels = [str(dec) + \"'s\" for dec in ybins[:-1]]\n",
    "\n",
    "df[\"decade\"] = pd.cut(df[\"year\"], ybins, labels = ylabels)\n",
    "\n",
    "token_list = []\n",
    "twenties = df[df[\"decade\"] == \"1920's\"]\n",
    "for i, count in enumerate(twenties[\"match_count\"]):\n",
    "    #     print(i)\n",
    "    for j in range(count):\n",
    "        token_list.append(twenties[\"ngram\"].iloc[i].split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GoC code - extracting ngrams by iterating through the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngram_extractor():\n",
    "    def __init__(self, dir_path, start_yr, end_yr, limit):\n",
    "        self.dir_path = dir_path\n",
    "        self.start_yr = start_yr\n",
    "        self.end_yr = end_yr\n",
    "        self.limit = limit\n",
    "    def __iter__(self): #Create generator for ngram counts - each ngram is multiplied by its counts. Easy on memory.\n",
    "        for filname in os.listdir():\n",
    "            with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "                for line in itertools.islice(fil, self.limit):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    ngram = line[0]\n",
    "                    if len(line)<3: #Why this component?\n",
    "                        continue\n",
    "                    try:\n",
    "                        year = int(line[1])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    if year > self.end_yr or year < self.start_yr:\n",
    "                        continue\n",
    "                    match_count = int(line[2])\n",
    "                    prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                    for i in range(match_count):\n",
    "                        yield prcssd_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the GoC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dir_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-efdf76af2cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAST_VERSION\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mngrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_yr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_yr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Using the recommended parameters according to Radim Rehurek\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dir_path' is not defined"
     ]
    }
   ],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1 # check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\n",
    "\n",
    "ngrams = ngram_extractor(dir_path, start_yr, end_yr, limit)\n",
    "\n",
    "#Using the recommended parameters according to Radim Rehurek\n",
    "model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, size=300, window=5, min_count=10, workers=10, hs=0, negative=8)\n",
    "model.save('w2vmodel_ng5_'+str(start_yr)+'_'+str(end_yr)+'_full')\n",
    "syn0_object=model.wv.syn0\n",
    "\n",
    "##output vector space##\n",
    "numpy.savetxt('syn0_ngf_'+str(start_yr)+'_'+str(end_yr)+'_full.txt',syn0_object,delimiter=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning ngrams according to decade and saving in folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngram_filer():\n",
    "    def __init__(self, dir_path, save_path, start_yr, end_yr, limit):\n",
    "        self.dir_path = dir_path\n",
    "        self.save_path = save_path\n",
    "        self.start_yr = start_yr\n",
    "        self.end_yr = end_yr\n",
    "        self.limit = limit\n",
    "        self.decades = [start_yr+i*10 for i in range((end_yr-start_yr)/10)]\n",
    "        \n",
    "    def __iter__(self): \n",
    "        write_dic = {}\n",
    "        for dec in self.decades:\n",
    "            fname = str(dec) + \".gz\"\n",
    "            write_dic[dec] = gzip.open(fname, \"wt\")\n",
    "            \n",
    "        for filname in os.listdir():\n",
    "            with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "                for line in itertools.islice(fil, self.limit):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    ngram = line[0]\n",
    "                    if len(line)<3: #Why this component?\n",
    "                        continue\n",
    "                    try:\n",
    "                        year = int(line[1])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                       u \n",
    "                    dec = math.floor(year/10)*10\n",
    "                    prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                    \n",
    "                    new_line = pr\n",
    "                    try:\n",
    "                        write_dic[dec].write('Contents of the example file go here.\\n')\n",
    "                    except:\n",
    "                        continue #is this what I want?\n",
    "                    for i in range(match_count):\n",
    "                        yield prcssd_ngram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini section of this for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA Foundation for Road Safety', '1990', '4', '2\\n'] aa foundation for road safety\t1990\t4\t2\n",
      "\n",
      "['AA Foundation for Road Safety', '1992', '1', '1\\n'] aa foundation for road safety\t1992\t1\t1\n",
      "\n",
      "['AA Foundation for Road Safety', '1993', '1', '1\\n'] aa foundation for road safety\t1993\t1\t1\n",
      "\n",
      "['AA Foundation for Road Safety', '1995', '1', '1\\n'] aa foundation for road safety\t1995\t1\t1\n",
      "\n",
      "['AA Foundation for Road Safety', '1996', '20', '8\\n'] aa foundation for road safety\t1996\t20\t8\n",
      "\n",
      "['AA Foundation for Road Safety', '1997', '3', '3\\n'] aa foundation for road safety\t1997\t3\t3\n",
      "\n",
      "['AA Foundation for Road Safety', '1998', '8', '3\\n'] aa foundation for road safety\t1998\t8\t3\n",
      "\n",
      "['AA Foundation for Road Safety', '1999', '3', '2\\n'] aa foundation for road safety\t1999\t3\t2\n",
      "\n",
      "['AA Foundation for Road Safety', '2000', '2', '1\\n'] aa foundation for road safety\t2000\t2\t1\n",
      "\n",
      "['AA Foundation for Road Safety', '2001', '12', '2\\n'] aa foundation for road safety\t2001\t12\t2\n",
      "\n",
      "Here\n",
      "aa foundation for road safety\t1990\t4\t\n",
      "aa foundation for road safety\t1992\t1\t\n",
      "aa foundation for road safety\t1993\t1\t\n",
      "aa foundation for road safety\t1995\t1\t\n",
      "aa foundation for road safety\t1996\t20\t\n",
      "aa foundation for road safety\t1997\t3\t\n",
      "aa foundation for road safety\t1998\t8\t\n",
      "aa foundation for road safety\t1999\t3\t\n",
      "aa foundation for road safety\t2000\t2\t\n",
      "aa foundation for road safety\t2001\t12\t\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import itertools\n",
    "import gzip\n",
    "import os\n",
    "import math\n",
    "\n",
    "os.chdir(\"C:/Users/user/Google Drive/KU/Thesis\")\n",
    "ng = \"googlebooks-eng-all-5gram-20120701-gp.gz\"\n",
    "ng = '-aa'\n",
    "fil = gensim.utils.open(ng)\n",
    "fil = gzip.open(ng, \"rt\")\n",
    "test_fil = gzip.open(\"test.gz\", \"wt\")\n",
    "for i,line in enumerate(fil):\n",
    "#     print(f\"jkl: {i}\", line)\n",
    "\n",
    "    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "    ngram = line[0]\n",
    "#     print(ngram)\n",
    "    if len(line)<3: #Why this component?\n",
    "        continue\n",
    "    try:\n",
    "        year = int(line[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    vol_count = [line[-1][:-2]]\n",
    "\n",
    "    dec = math.floor(year/10)*10\n",
    "    prcssd_ngram = [\" \".join([word.split(\"_\")[0] for word in ngram.lower().split()])] #Get rid of POS tagging on end of words\n",
    "    new_line = \"\\t\".join(prcssd_ngram + line[1:]) \n",
    "    if i <10:\n",
    "        print(line, new_line)\n",
    "    test_fil.write(new_line)\n",
    "    \n",
    "print(\"Here\")\n",
    "\n",
    "fil2 = gensim.utils.open(\"test.gz\")   \n",
    "test_fil = gzip.open(\"test.gz\", \"rt\")\n",
    "for i,line in enumerate(test_fil):\n",
    "#     line = line.split(\"\\n\")\n",
    "    if i <10:\n",
    "        print(line[:-2])\n",
    "    \n",
    "#     g_line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "    \n",
    "#     print(f\"***{i}***\", line)\n",
    "# prcssd_ngram = [word.split(\"_\")[0] for word in ng.lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:/google_ngrams/gb_12'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-b05b5e94bdff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:/google_ngrams/gb_12\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1-1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfil\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:/google_ngrams/gb_12'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"D:/google_ngrams/gb_12\")\n",
    "fil = gzip.open(\"1-1\", mode = \"rt\")\n",
    "count = 0\n",
    "for i,line in enumerate(itertools.islice(fil, 100000000)):\n",
    "    count += 1\n",
    "print(count)\n",
    "# colnames = [\"ngram\", \"year\", \"match_count\", \"vol_count\"]\n",
    "# df = pd.read_csv(\"1-0.gz\", sep='\\t', names=colnames, header=None )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time = ((30*5*100)/60)*2 \n",
    "# total_time/(2*5)\n",
    "total_time #250 for one GB decade 2.5k for 20th century\n",
    "(50*5)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=token_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"example_ngram.model\")\n",
    "# word_vec = model.wv\n",
    "# word_vec.most_similar(\"Government\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>match_count</th>\n",
       "      <th>page_count</th>\n",
       "      <th>volume_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Year, match_count, page_count, volume_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"googlebooks-eng-all-totalcounts-20120701.txt\", header = 0, names = [\"Year\", \"match_count\", \"page_count\", \"volume_count\"], sep = \",\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
