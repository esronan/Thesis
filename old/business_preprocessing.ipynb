{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text \n",
    "import regex\n",
    "import os\n",
    "import calendar\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from spellchecker import SpellChecker\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "specifics = {}\n",
    "specifics[\"hbr\"] = [\"hbr\", \"reprint\"]#\"article collection\"]  #numbers #punctuation\n",
    "months = list(calendar.month_name)\n",
    "new_content = []\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def clean_fil(fil):\n",
    "    txt = open(fil, encoding=\"utf-8\").read()\n",
    "    html2text.html2text(txt).replace(\"\\n\", \"\")\n",
    "    cleaned = \" \".join(html2text.html2text(txt).split())\n",
    "    #content = regex.findall(\"Full text of.+\", cleaned)[0]\n",
    "    try: \n",
    "        ind = regex.search(\"See other formats\", cleaned).end()\n",
    "        content = cleaned[ind+65:]\n",
    "    except:\n",
    "        print(f\"reg ex search on {fil} did not work\")\n",
    "        content = cleaned\n",
    "    \n",
    "    new_content = []\n",
    "    for token in content.split():\n",
    "        token = token.lower()\n",
    "        token = token.translate(table)\n",
    "        if token in specifics:\n",
    "            continue\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "        if token in months:\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        if not token.isalpha():\n",
    "            continue\n",
    "        if regex.match(\"ccc\", token):\n",
    "            continue\n",
    "        if regex.match(\"eee\", token):\n",
    "            continue\n",
    "        if not spell.known([token]):\n",
    "            continue\n",
    "        new_content.append(token)\n",
    "    final = \" \".join(new_content)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create string of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hbr_path = \"D:/google_ngrams/raw_data/hbr\"\n",
    "\n",
    "os.chdir(hbr_path)\n",
    "fils = os.listdir()\n",
    "full_string = \"\"\n",
    "for i,fil in enumerate(fils):\n",
    "    print(i)\n",
    "    txt = open(fil, encoding=\"utf-8\").read()\n",
    "    html2text.html2text(txt).replace(\"\\n\", \"\")\n",
    "    cleaned = \" \".join(html2text.html2text(txt).split())\n",
    "    content = regex.findall(\"Full text of.+\", cleaned)[0]\n",
    "    full_string += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_path = \"D:/google_ngrams/raw_data/forbes\"\n",
    "os.chdir(economist_path)\n",
    "fils = os.listdir()\n",
    "spell = SpellChecker()\n",
    "open(fils[6], \"r\", encoding=\"utf-8\").read()\n",
    "clean_fil(fils[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create string for each spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_writer(input_dir, output_dir, coll, overwrite=False):\n",
    "    if overwrite == True:\n",
    "        mode = \"w\"\n",
    "    else:\n",
    "        mode = \"a\"\n",
    "        \n",
    "    w_txts = {\"first\": open(output_dir + f\"first_spirit_{coll}.txt\", mode),\n",
    "              \"second\": open(output_dir + f\"second_spirit_{coll}.txt\", mode), \n",
    "              \"third\": open(output_dir + f\"third_spirit_{coll}.txt\", mode)}\n",
    "    \n",
    "    total_ln = 0\n",
    "    for fil in tqdm.tqdm(fils):\n",
    "        #clean\n",
    "        index = regex.findall(\"index\", fil)\n",
    "        if index:\n",
    "            continue # skip over index files\n",
    "        year = int(regex.findall(\"\\d{4}\", fil)[0])\n",
    "        content = clean_fil(fil)\n",
    "        ln = len(content.split())\n",
    "        total_ln += ln\n",
    "        \n",
    "        if year < 1930:\n",
    "            w_txts[\"first\"].write(content)\n",
    "        elif year < 1975:\n",
    "            w_txts[\"second\"].write(content)\n",
    "        else:\n",
    "            w_txts[\"third\"].write(content)\n",
    "\n",
    "    for key, val in w_txts.items():\n",
    "        val.close()\n",
    "    print(\"Cleaning and sorting complete. Total Length:\", total_ln)\n",
    "\n",
    "#8176 the last iter printed before memory error - so start from this index when starting again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economist2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████████████▉                                                     | 2683/9825 [56:31<2:49:46,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg ex search on sim_economist_1957-06-22_183_5939_djvu.txt did not work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 9825/9825 [6:17:51<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and sorting complete. Total Length: 1552317427\n",
      "forbes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████▊                                                           | 253/1140 [02:02<02:19,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg ex search on sim_forbes_1925-04-15_16_1djvu.txt did not work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1140/1140 [09:13<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and sorting complete. Total Length: 147619569\n",
      "bloomberg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 134/134 [01:31<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg ex search on sim_business-week_1941-05-31_613djvu.txt did not work\n",
      "Cleaning and sorting complete. Total Length: 24418819\n",
      "fortune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████▊                                                            | 67/293 [01:13<02:46,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg ex search on sim_fortune_1948-09_38_3djvu.txt did not work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 293/293 [05:49<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and sorting complete. Total Length: 90634832\n",
      "hbr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████▋      | 246/268 [03:03<00:11,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg ex search on sim_harvard-business-review_may-june-1954_32_3djvu.txt did not work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 268/268 [03:19<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and sorting complete. Total Length: 59197169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "colls =[\"economist2\", \"forbes\", \"bloomberg\", \"fortune\", \"hbr\"]\n",
    "for coll in colls:\n",
    "    print(coll)\n",
    "    input_dir = f\"D:/google_ngrams/raw_data/{coll}\"\n",
    "    os.chdir(input_dir)\n",
    "    fils = os.listdir()\n",
    "    output_dir = \"D:/google_ngrams/processed_data/business_text/\"\n",
    "    iter_writer(input_dir, output_dir, coll, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9866194.833333334"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 59197169/6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
