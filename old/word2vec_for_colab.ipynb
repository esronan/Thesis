{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HLYbERwurApA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim import downloader\n",
    "import gzip \n",
    "import math\n",
    "import itertools\n",
    "from time import time\n",
    "import os\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "colab = True\n",
    "\n",
    "if colab == True:\n",
    "  from google.colab import files\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive/')\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "  dir=\"/content/drive/MyDrive/google_ngrams/gb_12_processed_full\"\n",
    "else:\n",
    "    dir=\"D:/google_ngrams/gb_12_processed_filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sydTUDtErApF"
   },
   "outputs": [],
   "source": [
    " #Create generator for ngram counts - each ngram is multiplied by its counts. Easy on memory.\n",
    "class ngram_extractor():\n",
    "    def __init__(self, filname, extracted=False, limit=None):\n",
    "        self.limit = limit\n",
    "        self.filname = filname\n",
    "        self.extracted = extracted\n",
    "    def __iter__(self):\n",
    "      # file_list = drive.ListFile({'q': \"title contains '.gz'\"}).GetList() #shoehorn into my usecase!! source: https://colab.research.google.com/notebooks/snippets/drive.ipynb#scrollTo=-f-hfkapsiPc\n",
    "        start = time()\n",
    "        print(self.filname)\n",
    "        if self.extracted==False:\n",
    "            with gzip.open(self.filname, \"rt\", encoding=\"utf-8\") as fil: #just changed from r to rt so that it reads the csv separations\n",
    "            # with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "                for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    ngram = line[0]\n",
    "                    # if len(line)<3: #Why this component?\n",
    "                    #     continue\n",
    "                    # try:\n",
    "                    #     year = int(line[1])\n",
    "                    # except ValueError:\n",
    "                    #     continue\n",
    "                    # if year > self.end_yr or year < self.start_yr:\n",
    "                    #     continue\n",
    "                    match_count = int(line[2])\n",
    "                    prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                    for i in range(match_count):\n",
    "                        yield prcssd_ngram\n",
    "                print(f\"Time taken: {time()-start}\")\n",
    "        else: \n",
    "            with open(self.filname, \"rt\", encoding=\"utf-8\") as fil:\n",
    "                for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    ngram = line[0]\n",
    "                    # if len(line)<3: #Why this component?\n",
    "                    #     continue\n",
    "                    # try:\n",
    "                    #     year = int(line[1])\n",
    "                    # except ValueError:\n",
    "                    #     continue\n",
    "                    # if year > self.end_yr or year < self.start_yr:\n",
    "                    #     continue\n",
    "                    match_count = int(line[2])\n",
    "                    prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                    for i in range(match_count):\n",
    "                        yield prcssd_ngram\n",
    "                print(f\"Time taken: {time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bBgIiTcP8JJK"
   },
   "outputs": [],
   "source": [
    "class ngram_extractor_lite():\n",
    "    def __init__(self, filname, limit=None):\n",
    "        self.limit = limit\n",
    "        self.filname = filname\n",
    "    def __iter__(self):\n",
    "        start = time()\n",
    "        print(self.filname)\n",
    "        with gzip.open(self.filname, \"rt\", encoding=\"utf-8\") as fil: #just changed from r to rt so that it reads the csv separations\n",
    "        # with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "            for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                ngram = line[0].split()\n",
    "               # print(ngram)\n",
    "                # if len(line)<3: #Why this component?\n",
    "                #     continue\n",
    "                # try:\n",
    "                #     year = int(line[1])\n",
    "                # except ValueError:\n",
    "                #     continue\n",
    "                # if year > self.end_yr or year < self.start_yr:\n",
    "                #     continue\n",
    "                match_count = int(line[-1])\n",
    "                #prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                for i in range(match_count):\n",
    "                    yield ngram\n",
    "            print(f\"Time taken: {time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "5167b3e0554e48ae84e78d0973368a30",
      "f88a87153a7549839b61e7e361911fd3",
      "a65c1528b3974c8d99d850ddc5b61f94",
      "b45a5b6b2a8b4b4ba2fef6f623ae5707"
     ]
    },
    "id": "C0FZ2yXgrApJ",
    "outputId": "2d86e947-0f05-4129-d853-f4c21642d571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1500.gz', '1510.gz', '1520.gz', '1530.gz', '1540.gz', '1550.gz', '1560.gz', '1570.gz', '1580.gz', '1590.gz', '1600.gz', '1610.gz', '1620.gz', '1630.gz', '1640.gz', '1650.gz', '1660.gz', '1670.gz', '1680.gz', '1690.gz', '1700.gz', '1710.gz', '1720.gz', '1730.gz', '1740.gz', '1750.gz', '1760.gz', '1770.gz', '1780.gz', '1790.gz', '1800.gz', '1810.gz', '1820.gz', '1830.gz', '1840.gz', '1850.gz', '1860.gz', '1870.gz', '1880.gz', '1890.gz', '1900.gz', '1910.gz', '1920.gz', '1930.gz', '1940.gz', '1950.gz', '1960.gz', '1970.gz', '1980.gz', '1990.gz', '2000.gz', 'w2vmodel_ng5_1800.gz_full', 'w2vmodel_ng5_1980.gz_full.wv.vectors.npy', 'w2vmodel_ng5_1980.gz_full.syn1neg.npy', 'w2vmodel_ng5_1980.gz_full', 'w2vmodel_ng5_1970.gz_full.wv.vectors.npy', 'w2vmodel_ng5_1970.gz_full.syn1neg.npy', 'w2vmodel_ng5_1970.gz_full', 'w2vmodel_ng5_1900_full']\n",
      "1900.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40800bb747ef493e9dca1057350453b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1 # check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\n",
    "\n",
    "# %cd \"/content/drive/My\\ Drive/google_ngrams/gb_12_processed_full\" #for google colab\n",
    "# coll = \"gb_12_processed_3\"\n",
    "# os.chdir(\"D:/google_ngrams/gb_12_processed_3\")\n",
    "os.chdir(dir)\n",
    "print(os.listdir())\n",
    "\n",
    "# \"1900.gz\" - 4gb, ci - 45mb, cl - 100 mb\n",
    "filnames = [str(1900+10*i) + \".gz\" for i in range(10)]\n",
    "start = time()\n",
    "#Create generator\n",
    "for filname in filnames[::-1]:\n",
    "    ngrams = ngram_extractor_lite(filname=filname)\n",
    "\n",
    "    #Run model\n",
    "    #Using the recommended parameters according to Radim Rehurek. VECTOR_SIZE CHANGES TO SIZE IF USING GOOGLE COLAB\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, vector_size=300, window=5, min_count=25, workers=10, hs=0, negative=8) \n",
    "    #default epochs = 5, size = vector dimensions, min_count= min count for word to be considered, workers for multiprocessing, sg=1 = skipgram (0 = CBOW), hs=1 = softmax used, negative = no. noise words used in negative sampling\n",
    "    model.save('w2vmodel_ng5_'+ filname[:-3] +'_full')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_yE02XL1Ayp"
   },
   "outputs": [],
   "source": [
    "\n",
    "#an iterable version, so each iteration is ssaved (perhaps for each file there should be a save?):\n",
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "  if i > 0:\n",
    "    model = Word2Vec.load('w2vmodel_ng5_' + coll + \"_\" + str(start_yr)+'_'+str(end_yr)+str(-1))\n",
    "    model.train(ngrams,sg=1, size=300, window=5, min_count=10, workers=10, hs=0, negative=8, epochs=1)\n",
    "  else:\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, size=300, window=5, min_count=10, workers=10, hs=0, negative=8, epochs=1)\n",
    "    model.save('w2vmodel_ng5_'+ str(start_yr)+'_'+str(end_yr)+'_' + i)\n",
    "\n",
    "##output vector space##\n",
    "numpy.savetxt('syn0_ngf_'+str(start_yr)+'_'+str(end_yr)+'_full.txt',syn0_object,delimiter=\" \")\n",
    "\n",
    "print(f\"Time to download: {time() - start}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "word2vec_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
