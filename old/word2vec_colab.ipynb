{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HLYbERwurApA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim import downloader\n",
    "import gzip \n",
    "import math\n",
    "import itertools\n",
    "from time import time\n",
    "import os\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "colab = False\n",
    "\n",
    "if colab == True:\n",
    "  from google.colab import files\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive/')\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "  dir=\"/content/drive/MyDrive/google_ngrams/gb_12_processed_full\"\n",
    "else:\n",
    "    dir=\"D:/google_ngrams/us_12_latest8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sydTUDtErApF"
   },
   "outputs": [],
   "source": [
    " #Create generator for ngram counts - each ngram is multiplied by its counts. Easy on memory.\n",
    "class ngram_extractor():\n",
    "    def __init__(self, filname, extracted=False, limit=None):\n",
    "        self.limit = limit\n",
    "        self.filname = filname\n",
    "        self.extracted = extracted\n",
    "    def __iter__(self):\n",
    "      # file_list = drive.ListFile({'q': \"title contains '.gz'\"}).GetList() #shoehorn into my usecase!! source: https://colab.research.google.com/notebooks/snippets/drive.ipynb#scrollTo=-f-hfkapsiPc\n",
    "        start = time()\n",
    "        print(self.filname)\n",
    "        if self.extracted==False:\n",
    "            with gzip.open(self.filname, \"rt\", encoding=\"utf-8\") as fil: #just changed from r to rt so that it reads the csv separations\n",
    "            # with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "                for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    ngram = line[0]\n",
    "                    # if len(line)<3: #Why this component?\n",
    "                    #     continue\n",
    "                    # try:\n",
    "                    #     year = int(line[1])\n",
    "                    # except ValueError:\n",
    "                    #     continue\n",
    "                    # if year > self.end_yr or year < self.start_yr:\n",
    "                    #     continue\n",
    "                    match_count = int(line[2])\n",
    "                    prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                    for i in range(match_count):\n",
    "                        yield prcssd_ngram\n",
    "                print(f\"Time taken: {time()-start}\")\n",
    "        else: \n",
    "            with open(self.filname, \"rt\", encoding=\"utf-8\") as fil:\n",
    "                for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    ngram = line[0]\n",
    "                    # if len(line)<3: #Why this component?\n",
    "                    #     continue\n",
    "                    # try:\n",
    "                    #     year = int(line[1])\n",
    "                    # except ValueError:\n",
    "                    #     continue\n",
    "                    # if year > self.end_yr or year < self.start_yr:\n",
    "                    #     continue\n",
    "                    match_count = int(line[2])\n",
    "                    prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                    for i in range(match_count):\n",
    "                        yield prcssd_ngram\n",
    "                print(f\"Time taken: {time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bBgIiTcP8JJK"
   },
   "outputs": [],
   "source": [
    "class ngram_extractor_lite():\n",
    "    def __init__(self, filname, limit=None):\n",
    "        self.limit = limit\n",
    "        self.filname = filname\n",
    "    def __iter__(self):\n",
    "        start = time()\n",
    "        print(self.filname)\n",
    "        with gzip.open(self.filname, \"rt\", encoding=\"utf-8\") as fil: #just changed from r to rt so that it reads the csv separations\n",
    "        # with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "            for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                ngram = line[0].split()\n",
    "               # print(ngram)\n",
    "                # if len(line)<3: #Why this component?\n",
    "                #     continue\n",
    "                # try:\n",
    "                #     year = int(line[1])\n",
    "                # except ValueError:\n",
    "                #     continue\n",
    "                # if year > self.end_yr or year < self.start_yr:\n",
    "                #     continue\n",
    "                match_count = int(line[-1])\n",
    "                #prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                for i in range(match_count):\n",
    "                    yield ngram\n",
    "            print(f\"Time taken: {time()-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterator model\n",
    "Iterate through the decades and train/save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "5167b3e0554e48ae84e78d0973368a30",
      "f88a87153a7549839b61e7e361911fd3",
      "a65c1528b3974c8d99d850ddc5b61f94",
      "b45a5b6b2a8b4b4ba2fef6f623ae5707"
     ]
    },
    "id": "C0FZ2yXgrApJ",
    "outputId": "2d86e947-0f05-4129-d853-f4c21642d571",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_12756/3906183540.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAST_VERSION\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# %cd \"/content/drive/My\\ Drive/google_ngrams/gb_12_processed_full\" #for google colab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# coll = \"gb_12_processed_3\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# os.chdir(\"D:/google_ngrams/gb_12_processed_3\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1 # check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\n",
    "\n",
    "# %cd \"/content/drive/My\\ Drive/google_ngrams/gb_12_processed_full\" #for google colab\n",
    "# coll = \"gb_12_processed_3\"\n",
    "# os.chdir(\"D:/google_ngrams/gb_12_processed_3\")\n",
    "os.chdir(dir)\n",
    "print(os.listdir())\n",
    "\n",
    "# \"1900.gz\" - 4gb, ci - 45mb, cl - 100 mb\n",
    "filnames = [str(1900+10*i) + \".gz\" for i in range(10)]\n",
    "start = time()\n",
    "method = \"sg\" #cbow or skip-gram\n",
    "#Create generator\n",
    "for filname in filnames:\n",
    "    ngrams = ngram_extractor_lite(filname=filname)\n",
    "    if method == \"cbow\":\n",
    "        sg = 0\n",
    "    else:\n",
    "        sg = 1\n",
    "    #Run model\n",
    "    #Using the recommended parameters according to Radim Rehurek. VECTOR_SIZE CHANGES TO SIZE IF USING GOOGLE COLAB\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=sg, vector_size=300, window=5, min_count=25, workers=10, hs=0, negative=8) \n",
    "    #default epochs = 5, size = vector dimensions, min_count= min count for word to be considered, workers for multiprocessing, sg=1 = skipgram (0 = CBOW), hs=1 = softmax used, negative = no. noise words used in negative sampling\n",
    "    model.save('w2vmodel_ng5_'+ method + \"_\" + filname[:-3] +'_full')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterable version - one epoch at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_yE02XL1Ayp"
   },
   "outputs": [],
   "source": [
    "\n",
    "#an iterable version, so each iteration is ssaved (perhaps for each file there should be a save?):\n",
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "  if i > 0:\n",
    "    model = Word2Vec.load('w2vmodel_ng5_' + coll + \"_\" + str(start_yr)+'_'+str(end_yr)+str(-1))\n",
    "    model.train(ngrams,sg=1, size=300, window=5, min_count=10, workers=10, hs=0, negative=8, epochs=1)\n",
    "  else:\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, size=300, window=5, min_count=10, workers=10, hs=0, negative=8, epochs=1)\n",
    "    model.save('w2vmodel_ng5_'+ str(start_yr)+'_'+str(end_yr)+'_' + i)\n",
    "\n",
    "##output vector space##\n",
    "numpy.savetxt('syn0_ngf_'+str(start_yr)+'_'+str(end_yr)+'_full.txt',syn0_object,delimiter=\" \")\n",
    "\n",
    "print(f\"Time to download: {time() - start}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "word2vec_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
