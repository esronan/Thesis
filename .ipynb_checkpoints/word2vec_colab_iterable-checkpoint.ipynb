{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HLYbERwurApA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim import downloader\n",
    "import gzip \n",
    "import math\n",
    "import itertools\n",
    "from time import time\n",
    "import os\n",
    "import tqdm.notebook as tq\n",
    "import mgzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False\n",
    "\n",
    "if colab == True:\n",
    "  from google.colab import files\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive/')\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "  dir=\"/content/drive/MyDrive/google_ngrams/gb_12_processed_full\"\n",
    "else:\n",
    "    dir=\"D:/google_ngrams/us_12_latest8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bBgIiTcP8JJK"
   },
   "outputs": [],
   "source": [
    "class ngram_extractor_lite():\n",
    "    def __init__(self, filname, limit=None):\n",
    "        self.limit = limit\n",
    "        self.filname = filname\n",
    "    def __iter__(self):\n",
    "        start = time()\n",
    "        print(self.filname)\n",
    "        with mgzip.open(self.filname, \"rt\", encoding=\"utf-8\", thread= 4) as fil: #just changed from r to rt so that it reads the csv separations\n",
    "        # with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "            j = 0\n",
    "            for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                ngram = line[0].split()\n",
    "                match_count = int(line[-1])\n",
    "               \n",
    "                for i in range(match_count):\n",
    "                    yield ngram\n",
    "            print(f\"Time taken: {time()-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterable version - one epoch at at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_yE02XL1Ayp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9dc74a7124464ba38fc7bddb8c3b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1 # check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\n",
    "\n",
    "#an iterable version, so each iteration is ssaved (perhaps for each file there should be a save?):\n",
    "epochs = 5\n",
    "vocab = False\n",
    "completed = 0\n",
    "method = 1 # 1  sg, cbow = 0\n",
    "\n",
    "size = 300 # vector size\n",
    "min_count = 25\n",
    "data_dir = \"D:/google_ngrams/processed_data/gb_12_min_100\"\n",
    "out_dir = \"D:/google_ngrams/vectors/gb_12_min_100/\"\n",
    "os.chdir(data_dir)\n",
    "filname = \"1800.gz\"\n",
    "ngrams = ngram_extractor_lite(filname=filname)\n",
    "for i in range(epochs-completed):\n",
    "    if vocab:\n",
    "        model = Word2Vec.load('../vectors/us_12/w2vmodel_ng5_' + filname  + \"_\" + str(\"vocab\"))\n",
    "        print(\"Vocab loaded.\")\n",
    "    elif completed != 0:\n",
    "        model = Word2Vec.load('w2vmodel_ng5_' + filname + \"_\" + str(completed))\n",
    "        print(\"Model loaded.\")\n",
    "    elif i == 0:\n",
    "        model = gensim.models.word2vec.Word2Vec(sg=method, vector_size=size, window=5, min_count=25, workers=10, hs=0, negative=8, epochs=1)\n",
    "        model.build_vocab(ngrams)\n",
    "        print(\"Vocab built.\")\n",
    "        model.save('w2vmodel_ng5_'+ filname +'_' + str(\"vocab\"))\n",
    "    model.train(ngrams, total_examples = model.corpus_count, epochs = 1)\n",
    "    model.save('w2vmodel_ng5_'+ filname +'_' + str(completed+i+1))\n",
    "    print(\"Epoch \" + str(completed + i + 1) + \" completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1 # check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\n",
    "\n",
    "#an iterable version, so each iteration is ssaved (perhaps for each file there should be a save?):\n",
    "epochs = 5\n",
    "vocab = True\n",
    "completed = 0\n",
    "method = 1 # 1  sg, cbow = 0\n",
    "size = 300 # vector size\n",
    "min_count = 25\n",
    "data_dir = \"D:/google_ngrams/us_12_latest8\"\n",
    "out_dir = \"D:/google_ngrams/vectors/us_12/\"\n",
    "os.chdir(data_dir)\n",
    "print(os.listdir())\n",
    "filname = \"1920.gz\"\n",
    "ngrams = ngram_extractor_lite(filname=filname)\n",
    "for i in range(epochs-completed):\n",
    "    if vocab:\n",
    "        model = Word2Vec.load('../vectors/us_12/w2vmodel_ng5_' + filname + \"_\" + str(\"vocab\"))\n",
    "        print(\"Vocab loaded.\")\n",
    "    elif completed != 0:\n",
    "        model = Word2Vec.load('w2vmodel_ng5_' + filname + \"_\" + str(completed))\n",
    "        print(\"Model loaded.\")\n",
    "    elif i == 0:\n",
    "        model = gensim.models.word2vec.Word2Vec(sg=method, vector_size=size, window=5, min_count=25, workers=10, hs=0, negative=8, epochs=1)\n",
    "        model.build_vocab(ngrams)\n",
    "        print(\"Vocab built.\")\n",
    "        model.save('w2vmodel_ng5_'+ filname +'_' + str(\"vocab\"))\n",
    "    model.train(ngrams, total_examples = model.corpus_count, epochs = 1)\n",
    "    model.save('w2vmodel_ng5_'+ filname +'_' + str(completed+i+1))\n",
    "    print(\"Epoch \" + str(completed + i + 1) + \" completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "word2vec_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
