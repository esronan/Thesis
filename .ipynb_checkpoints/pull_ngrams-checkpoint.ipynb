{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating list of URLS per ngram collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import re as reg\n",
    "#Creating a list of URLs for each of the Google Books corpora\n",
    "ngram_urls = {}\n",
    "\n",
    "#2020 English - all (both British & American) \n",
    "num_files = 19423\n",
    "ngram_urls[\"eng_all_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng/5-{}-of-19423.gz\".format(i)\n",
    "  for i in range(num_files)]\n",
    "\n",
    "#2012 english \n",
    "alph_list = []\n",
    "num_files = 1000\n",
    "alph = \"_\" + string.ascii_lowercase\n",
    "\n",
    "\n",
    "for i in range(num_files)[27:]: \n",
    "    try:\n",
    "        alph_list.append(alph[i//27] + alph[i%27])\n",
    "    except:\n",
    "        pass\n",
    "#All suffixes - lacking \n",
    "#turn below into a list and add the above\n",
    "all_suff = alph_list + list(string.digits) + [\"_ADJ_\", \"_ADP_\", \"_ADV_\", \"_CONJ_\", \"_DET_\", \"_NOUN_\", \"_NUM_\", \"_PRON_\", \"_PRT_\", \"_VERB_\"]       \n",
    "\n",
    "#All english 2012\n",
    "ngram_urls[\"eng_all_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n",
    "\n",
    "#English one million\n",
    "ngram_urls[\"one_mill\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-1M-5gram-20090715-{}.csv.zip\".format(i) for i in range(799)]\n",
    "\n",
    "#British English 2020\n",
    "ngram_urls[\"gb_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-gb/5-{}-of-03098.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(3098)]\n",
    "\n",
    "#British English 2020\n",
    "ngram_urls[\"gb_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-gb/5-{}-of-03098.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(3098)]\n",
    "\n",
    "#British English 2012\n",
    "ngram_urls[\"gb_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-gb-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n",
    "\n",
    "#American English 2020\n",
    "ngram_urls[\"us_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-us/5-{}-of-11145.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(11145)]\n",
    "\n",
    "#American English 2012\n",
    "ngram_urls[\"us_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-us-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n",
    "\n",
    "#English Fiction 2020\n",
    "ngram_urls[\"eng_fic_20\"] = [\"http://storage.googleapis.com/books/ngrams/books/20200217/eng-fiction/5-{}-of-01449.gz\".format(\"0\"*(5-len(str(i)))+ str(i)) for i in range(1449)]\n",
    "\n",
    "#English Fiction 2012\n",
    "ngram_urls[\"eng_fic_12\"] = [\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-fiction-all-5gram-20120701-{}.gz\".format(i) for i in all_suff]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to download a specified collection (coll) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/google_ngrams/gb_12/a_\n"
     ]
    }
   ],
   "source": [
    "#https://likegeeks.com/downloading-files-using-python/\n",
    "#PBAR: https://towardsdatascience.com/how-to-download-files-using-python-part-2-19b95be4cdb5\n",
    "import requests as re\n",
    "from time import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "# from tqdm import tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "# url = \"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-5gram-20120701-a_.gz\"\n",
    "# file = re.get(url)\n",
    "# open(file).write(file.content)\n",
    "\n",
    "#CHANGE THE PATH TO FOLDER ON HARDDRIVE\n",
    "def url_dl(coll, url, path, it):\n",
    "    if coll[-2:] == \"12\":\n",
    "        name = reg.search(\"01-.+.gz\", url).start()\n",
    "        path = path + url[name+3:-3]\n",
    "        \n",
    "    print(path)\n",
    "    r = re.get(url, stream=True)\n",
    "#     r.raise_for_status()\n",
    "    total_size = int(r.headers.get('content-length'))\n",
    "    initial_pos = 0\n",
    "    with open(path, \"wb\") as f: \n",
    "        for chunk in r.iter_content(chunk_size=8192):  \n",
    "            f.write(chunk)\n",
    "    del r\n",
    "\n",
    "hd_path = \"D:/google_ngrams/\"\n",
    "start = time()\n",
    "\n",
    "coll = \"gb_12\"  \n",
    "colls = [\"us_12\", \"eng_all_12\", \"eng_fic_12\",  \"one_mill\"] # \"eng_fic_20\"\n",
    "os.chdir(hd_path)\n",
    "os.mkdir(coll)\n",
    "for i,url in enumerate(ngram_urls[coll]):\n",
    "    file_path = hd_path + coll + \"/\"\n",
    "    url_dl(coll, url, file_path, str(i))\n",
    "    print(f\"{url[-8:]}\\nIteration: {i} of {len(ngram_urls[coll])}, time taken: {time() - start}\")\n",
    "\n",
    "# iterating through all collections\n",
    "# for coll in colls:\n",
    "#     for i,url in enumerate(ngram_urls[coll]):\n",
    "#         print(f\"Iteration: {i} of {len(ngram_urls[coll])}, time taken: {time() - start}\")\n",
    "#         file_path = hd_path + coll + \"/\"\n",
    "#         url_dl(url, file_path)\n",
    "\n",
    "print(f\"Time to download: {time() - start}\") # I think this will just print out at end?\n",
    "# ThreadPool(9).imap_unordered(url_dl, ngram_urls[\"one_mill\"][:100]) #for multiple URLs\n",
    "#urllib.request.urlretrieve('url', 'path') using urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n",
    "# r.raise_for_status()\n",
    "#         with open(local_filename, 'wb') as f:\n",
    "#             for chunk in r.iter_content(chunk_size=8192): \n",
    "#                 # If you have chunk encoded response uncomment if\n",
    "#                 # and set chunk_size parameter to None.\n",
    "#                 #if chunk: \n",
    "#                 f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset sizes \n",
    "- English One Million: 31.98 gig\n",
    "- eng_all_20: ?\n",
    "- gb_20: 1845gb\n",
    "- us_20: ?\n",
    "- eng_all_12: 3.67 gb\n",
    "- us12: 160gb\n",
    "- gb12: 52.6gb. Time to download: 2126.120045900345\n",
    "- eng_fic20: 659gb\n",
    "- eng_fic12: 23.4gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to find size of each collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695\n",
      "52.660931616\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests as re\n",
    "\n",
    "# sum([re.head(\"http://storage.googleapis.com/books/ngrams/books/20200217/eng/5-1-of-19423.gz\")[\"Content-length\"] for url in ngram_urls[\"one_mill\"]])\n",
    "size = 0\n",
    "for i, url in enumerate(ngram_urls[\"gb_12\"]):\n",
    "    \n",
    "    f = re.head(url)\n",
    "    size = size + int(f.headers[\"Content-length\"])\n",
    "    if i == len(ngram_urls[\"gb_12\"])-1:\n",
    "        print(i)\n",
    "        print(size/10**9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
