{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c77182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install gensim\n",
    "#!pip install mgzip\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim import downloader\n",
    "import gzip \n",
    "import math\n",
    "import itertools\n",
    "from time import time\n",
    "import os\n",
    "import tqdm as tq\n",
    "import mgzip\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953a0bf",
   "metadata": {},
   "source": [
    "# Set up loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        logging.debug('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        log_fil.write('Loss after epoch {}: {}\\n\\n'.format(self.epoch, loss_now))\n",
    "        losses[filname][self.epoch] = loss_now\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2886f",
   "metadata": {},
   "source": [
    "# Ngram generator from gzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4053141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngram_extractor_lite():\n",
    "    def __init__(self, filname, blank=False, limit=None):\n",
    "        self.limit = limit\n",
    "        self.filname = filname\n",
    "        self.blank = blank\n",
    "    def __iter__(self):\n",
    "        start = time()\n",
    "        print(\"n\", self.filname)\n",
    "        with mgzip.open(self.filname, \"rt\", encoding=\"utf-8\", thread= 4) as fil: #just changed from r to rt so that it reads the csv separations\n",
    "        # with gensim.utils.smart_open(os.path.join(self.dir_path, filname)) as fil:\n",
    "            j = 0\n",
    "            for line in tq.tqdm(itertools.islice(fil, self.limit)):\n",
    "                #j+=1\n",
    "                #if j > self.blank:\n",
    "                 #   break\n",
    "                line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                ngram = line[0].split()\n",
    "                if len(ngram) < 3:\n",
    "                    continue\n",
    "                match_count = int(line[-1])\n",
    "                #prcssd_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()] #Get rid of POS tagging on end of words\n",
    "                for i in range(match_count):\n",
    "                    yield ngram\n",
    "            t = time()-start\n",
    "            print(f\"Time taken: {t // 3600}H, {round((t % 3600)/60, 2)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2sent(fil):\n",
    "    f = open(fil, \"r\")\n",
    "    sents = []\n",
    "    for line in tq.tqdm(itertools.islice(f)):\n",
    "        line = gensim.utils.to_unicode(line)\n",
    "        \n",
    "    return sents\n",
    "os.chdir(\"D:/google_ngrams/processed_data/business_text\")\n",
    "f = open(os.listdir()[0], \"r\").readlines()\n",
    "sents = []\n",
    "i = 0\n",
    "for line in tq.tqdm(f):\n",
    "    line = gensim.utils.to_unicode(line)\n",
    "    print(line[:150])\n",
    "    i +=1\n",
    "    if i > 10:\n",
    "        break\n",
    "print(os.listdir()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a360304",
   "metadata": {},
   "source": [
    "# Find where in the files the blanks (Pure POS tags) begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcce1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_blanks(dir):\n",
    "    ''' Create two dictionaries, one containing the index of the first blank line in each file,\n",
    "    and the second containing the length of the files'''\n",
    "    if dir[-1] != \"/\":\n",
    "        print(\"Add slash to end of dir.\")\n",
    "        return None\n",
    "    filnames = [f\"19{i}0.gz\" for i in range(0,10)]\n",
    "    infos = {\"blank_ind\": {}, \"length\": {}}\n",
    "    os.chdir(dir)\n",
    "    try:\n",
    "        infos = pd.read_csv(dir + \"file_infos.csv\")\n",
    "        infos = infos.set_index(\"filename\")\n",
    "        print(\"Infos loaded.\")\n",
    "        return infos\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        (\"Calculating infos...\")\n",
    "        for fil in filnames[:1]:\n",
    "            print(fil)\n",
    "            with gzip.open(fil, \"rt\", encoding = 'utf-8') as f:\n",
    "                lst = []\n",
    "                for line in f:\n",
    "                    spl = line.split()\n",
    "                    lst.append(spl)\n",
    "            for i,spl in enumerate(lst[::-1]):\n",
    "                #print(\"spl\", spl, \"eln\", len(spl))\n",
    "                if len(spl) > 2:\n",
    "                    infos[\"blank_ind\"][fil] = len(lst)-(i+1)\n",
    "                    infos[\"length\"][fil] = len(lst)\n",
    "                    break\n",
    "        infos = pd.DataFrame(infos)#.to_csv(dir + \"file_infos\")\n",
    "        infos.index.name=\"filename\"\n",
    "        print(infos)\n",
    "        infos.to_csv(dir + \"file_infos.csv\")\n",
    "    return infos\n",
    "infos = find_blanks(\"D:/google_ngrams/processed_data/gb_12_full_full/\")\n",
    "#TO ADD: ngram counts, averages, etc - everythign in expoloratory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#infos = infos.set_index(\"filename\")\n",
    "infos[\"blank_ind\"][\"1900.gz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9187e2",
   "metadata": {},
   "source": [
    "# Iterable version (no loss calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7941dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1 # check that it is the latest version of Word2vecngrams = ngram_extractor(dir_path, save_path, start_yr, end_yr, limit)\n",
    "\n",
    "#an iterable version, so each iteration is ssaved (perhaps for each file there should be a save?):\n",
    "epochs = 5\n",
    "vocab = False\n",
    "completed = 0\n",
    "method = 1 # 1  sg, cbow = 0\n",
    "\n",
    "size = 300 # vector size\n",
    "min_count = 25\n",
    "filnames = [f\"19{i}0.gz\" for i in range(0,10)]\n",
    "\n",
    "os.chdir(\"D:/google_ngrams/processed_data/gb_12_full_full\")\n",
    "\n",
    "for filname in filnames[:1]:\n",
    "    ngrams = ngram_extractor_lite(filname=filname)#, blank=infos[\"blank_id\"][filname])\n",
    "    for i in range(epochs-completed):\n",
    "        if vocab:\n",
    "            model = Word2Vec.load('w2vmodel_ng5_' + filname  + \"_\" + str(\"vocab\"))\n",
    "            print(\"Vocab loaded.\")\n",
    "        elif completed != 0:\n",
    "            model = Word2Vec.load('w2vmodel_ng5_' + filname + \"_\" + str(completed))\n",
    "            print(\"Model loaded.\")\n",
    "        elif i == 0:\n",
    "            model = gensim.models.word2vec.Word2Vec(sg=method, vector_size=size, window=5, sg=1 min_count=25, workers=10, hs=0, negative=8, epochs=1, sample=10000, compute_loss=True, callbacks = [callback()])\n",
    "            model.build_vocab(ngrams)\n",
    "            print(\"Vocab built.\")\n",
    "            model.save('w2vmodel_ng5_'+ filname +'_' + str(\"vocab\"))\n",
    "        model.train(ngrams, total_examples = model.corpus_count, epochs = 1)\n",
    "        model.save('w2vmodel_ng5_'+ filname +'_' + str(completed+i+1))\n",
    "        print(\"Epoch \" + str(completed + i + 1) + \" completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43f560",
   "metadata": {},
   "source": [
    "# Standard version - with loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86a0099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"D:/google_ngrams/processed_data/business_text\")\n",
    "output_dir = \"D:/google_ngrams/Vectors/business_vectors\"\n",
    "\n",
    "#filnames = [f\"19{i}0.gz\" for i in range(0,10)] \n",
    "colls =[\"economist2\", \"forbes\", \"bloomberg\", \"fortune\", \"hbr\"]\n",
    "filnames = [\"first\", \"second\", \"third\"]\n",
    "\n",
    "#filnames = [f\"first_spirit_{coll}\" for coll in colls]\n",
    "losses = {filname: {} for filname in filnames}\n",
    "for filname in filnames:\n",
    "    logging.basicConfig(filename=f'{output_dir}/{filname}.log', encoding='utf-8', level=logging.DEBUG)\n",
    "    log_fil = open(f'{output_dir}/{filname}_log.txt', mode = 'w')\n",
    "    #ngrams = ngram_extractor_lite(filname=filname)#, blank=infos[\"blank_ind\"][filname])\n",
    "    txts = [f\"{filname}_spirit_{coll}.txt\" for coll in colls]\n",
    "    ngrams = []\n",
    "    for txt in txts:\n",
    "        content = file2sent(txt)\n",
    "        ngrams += content\n",
    "        print(ngrams[:10])\n",
    "        del content\n",
    "    print(\"Starting training...\")\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, vector_size=300, window=5, min_count=5, compute_loss=True, callbacks = [callback()], workers=10, hs=0, negative=8) \n",
    "    #default epochs = 5, size = vector dimensions, min_count= min count for word to be considered, workers for multiprocessing, sg=1 = skipgram (0 = CBOW), hs=1 = softmax used, negative = no. noise words used in negative sampling\n",
    "    model.save(output_dir + \"w2vmodel_ng5_sg_\" + filname +'_full')\n",
    "    pd.DataFrame(losses).to_csv(f\"{output_dir}/{filname}_losses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:/google_ngrams/processed_data/us_12_full_full\")\n",
    "output_dir = \"D:/google_ngrams/Vectors/us_12_final/\"\n",
    "\n",
    "filnames = [f\"19{i}0.gz\" for i in range(0,10)] \n",
    "#colls =[\"economist2\", \"forbes\", \"bloomberg\", \"fortune\", \"hbr\"]\n",
    "#filnames = [\"first\", \"second\", \"third\"]\n",
    "\n",
    "#filnames = [f\"first_spirit_{coll}\" for coll in colls]\n",
    "losses = {filname: {} for filname in filnames}\n",
    "for filname in filnames:\n",
    "    logging.basicConfig(filename=f'{output_dir}/{filname}.log', encoding='utf-8', level=logging.DEBUG)\n",
    "    log_fil = open(f'{output_dir}/{filname}_log.txt', mode = 'w')\n",
    "    ngrams = ngram_extractor_lite(filname=filname)#, blank=infos[\"blank_ind\"][filname])\n",
    "    #txts = [f\"{filname}_spirit_{coll}.txt\" for coll in colls]\n",
    "    #ngrams = []\n",
    "    #for txt in txts:\n",
    "    #    content = file2sent(txt)\n",
    "    #    ngrams += content\n",
    "    #    print(ngrams[:10])\n",
    "    #    del content\n",
    "    print(\"Starting training...\")\n",
    "    model = gensim.models.word2vec.Word2Vec(ngrams,sg=1, vector_size=300, window=5, min_count=5, compute_loss=True, callbacks = [callback()], workers=10, hs=0, negative=8) \n",
    "    #default epochs = 5, size = vector dimensions, min_count= min count for word to be considered, workers for multiprocessing, sg=1 = skipgram (0 = CBOW), hs=1 = softmax used, negative = no. noise words used in negative sampling\n",
    "    model.save(output_dir + \"w2vmodel_ng5_sg_\" + filname +'_full')\n",
    "    pd.DataFrame(losses).to_csv(f\"{output_dir}/{filname}_losses.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
